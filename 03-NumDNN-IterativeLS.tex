\documentclass[12pt,fleqn]{beamer}
\usepackage{verbatim}

\input{beamerStyle.tex}
\input{abbrv.tex}


\title[Iterative LS]{Solving the Least-Squares Problem in Practice}
\subtitle{Numerical Methods for Deep Learning}
\date{}

\begin{document}

\makebeamertitle

% section bias_variance (end)

\section{Iterative Methods} % (fold)
\label{sec:iterative_methods}
\begin{frame}\frametitle{Iterative Solvers for Least-Squares Regression}
	Last time: Given $\bfY\in \R^{n_f\times n}$ and $\bfC\in\R^{n_c\times n}$, solve
	$$
	\min_{\bfX}  \frac12\left\|  \bfA   \bfX  -  \bfC^\top \right\|_F^2
	$$
	directly using $\bfX^* = (\bfA^{\top} \bfA)^{-1}\bfA^{\top} \bfC^\top$. Here
	$$
		\bfA = \begin{pmatrix}
			\bfY^\top && \bfe_n
		\end{pmatrix}, \quad \text{ and } \quad 
		\bfX = \bfW^\top \in \R^{(n_f+1)\times n_c}.
	$$
	Problems: Generating  $\bfA^{\top}\bfA$ and solving normal equations is too costly for large-scale problems.
	
	\pause

	\bigskip
	
Today: Iterative methods that avoid working with $\bfA^{\top}\bfA$ 
\begin{itemize}
\item Steepest descent 
\item Conjugate gradient for least-squares (CGLS)
\end{itemize}
Excellent references: Numerical Optimization~\cite{NocedalWright2006}, iterative linear algebra~\cite{Saad2003}, general introduction \cite{AscherGreif2011}
\end{frame}

\begin{frame}\frametitle{Iterative Methods}


General idea - obtain a sequence $\bfX_1, \ldots,\bfX_j,\ldots$
that converges to least-squares solution $\bfX^*$

$$
	\bfX_j \longrightarrow \bfX^*, \quad \text{ for } \quad j \to \infty.
$$

\bigskip

How fast does the sequence converge? Assume
$$
\|\bfX_{j+1} - \bfX^* \| < \gamma_j \|\bfX_{j} - \bfX^* \|
$$ 
where all $\gamma_j < 1$. Then

\begin{itemize}
\item If $\gamma_j$ is bounded away from $0$ and 1 the convergence is linear
\item If $\gamma_j \rightarrow 0$ the convergence is superlinear
\item If $\gamma_j \rightarrow 1$ the convergence is sublinear
\end{itemize}

The sequence converges quadratically if $\gamma_j$ is bounded away from 0 and 1 and 
$$ \|\bfX_{j+1} - \bfX^* \| < \gamma_j \|\bfX_{j} - \bfX^* \|^2 $$ 
\end{frame}

\input{slides/sd.tex}

\begin{frame}\frametitle{Steepest Descent for Least-Squares}

Consider now
$$ \phi(\bfx) = \hf \|\bfA \bfx - \bfc\|^2 \quad \text{ with } \quad \grad_{\bfx} \phi(\bfx) = \bfA^{\top}(\bfA \bfx -  \bfc).$$

\bigskip

Steepest descent direction is  $\bfd_j = \bfA^{\top}(\bfc - \bfA \bfx_j)$ and
$$ \bfx_{j+1} = \bfx_j + \alpha_j \bfd_j $$

How to choose $\alpha_j$? 
\pause
Idea: Minimize $\phi$ along direction $\bfd_j$
$$ 
\alpha_j = \argmin_\alpha \phi(\bfx_j + \alpha \bfd_j) = 
 \argmin_\alpha \hf \|\alpha \bfA \bfd_j - \bfr_j\|^2
$$
with residual $\bfr_j = \bfc - \bfA \bfx_j $.

\bigskip
\pause

This leads to simple quadratic equation in 1D whose solution is
 
  
$$ \alpha_j = {\frac { \bfr_j^{\top}\bfA\bfd_j}{\|\bfA \bfd_j\|^2}} $$


\end{frame}


\begin{frame}\frametitle{Algorithm: Steepest Descent for Least-Squares}


for $j=1,\ldots $
\begin{itemize}
\item Compute residual $\bfr_j = \bfc - \bfA \bfx_j$
\item Compute the SD direction $\bfd_j = \bfA^{\top}\bfr_j$
\item Compute step size  $\alpha_j = {\frac { \bfr_j^{\top}\bfA\bfd_j }{\|\bfA \bfd_j\|^2}}$
\item Take the step $\bfx_{j+1} = \bfx_j + \alpha_j \bfd_j$
\end{itemize}

\pause

Converges linearly, i.e., 
$$
	\| \bfX_{j+1} - \bfX^*\| < \gamma \|\bfX_j - \bfX^* \| \quad \text{ with } \quad \gamma \approx \left| {\frac {\kappa-1}{\kappa+1}} \right|
$$

Here, $\kappa$ depends on condition number of $\bfA$, i.e., 
$$ \kappa \approx {\frac {\sigma_{\bf min}^2}{\sigma_{\bf max}^2}} $$


Can be painfully slow for ill-conditioned problems

\end{frame}


\begin{frame}\frametitle{Accelerating Steepest Descent: Post-Conditioning}

\begin{columns}
	\column{.7\textwidth}
	
	Idea: Improve convergence by transforming the problem
	 $$ \phi(\bfx) = \hf \|\bfA \bfS \bfS^{-1}\bfx - \bfc \|^2$$
	
	Here: $\bfS$ is invertible

	Solve in two steps:
	\begin{enumerate}
		\item Set $\bfz = \bfS^{-1}\bfx$ and compute
	 $$ \bfz^* \argmin_{\bfz} \hf \|\bfA \bfS \bfz - \bfc \|^2$$
	 	\item Then $\bfx = \bfS \bfz$.
	\end{enumerate} 

	 \bigskip

	 Pick $\bfS$ such that $\bfA \bfS$ is better conditioned.

	\column{.3\textwidth}
	\pause
	\begin{tabular}{@{}c@{}}
		original problem: \\
		\includegraphics[width=35mm]{convSD2DQuadratic}\\
		post-conditioned:\\
		\includegraphics[width=35mm]{convSD2DQuadraticGood} \\
	\end{tabular}
\end{columns}

\end{frame}



\begin{frame}\frametitle{Exercise: Steepest Descent for Least-Squares}

Goal: Program steepest descent and solve a simple problem.

\bigskip

To verify your code generate data using
$$ \bfc = \bfA \bfx_{\rm true} + \bfepsilon. $$
where $\bfepsilon$ is random with zero mean and standard deviation $0.1$ and
$$
 \bfY = \begin{pmatrix}  1 & 1+a \\ 1 & 1+2a \\ 1 & 1+3a
	\end{pmatrix} 
	\quad  \text{ and } \quad
	 \bfw_{\rm true} = \begin{pmatrix} 1\\ 1.2 \end{pmatrix}.
$$

\bigskip

Plot errors $\|\bfx_j - \bfx^*\|$ for $j=1,\dots$ and $a \in \{1, 10^{-2}, 10^{-5}\}$.

\end{frame}



\begin{frame}\frametitle{Conjugate Gradient Method for Least-Squares}

CG is designed to solve quadratic optimization problems 
$$\min_\bfx \hf \bfx^{\top} \bfH \bfx - \bfb^{\top} \bfx$$
with $\bfH$ symmetric positive definite. In our case

$$ \argmin_\bfx \hf \|\bfA\bfx - \bfc\|^2 = \argmin_\bfx \hf \bfx^{\top} \underbrace{\bfA^{\top}\bfA}_{=\bfH} \bfx - \underbrace{\bfc^{\top}\bfA}_{=\bfb^\top}\bfx $$

\bigskip
 

CG improves over SD by using previous step (not a memory-less method) and constructing a basis for the solution.

\bigskip

Facts:
\begin{itemize}
	\item terminates after at most $n$ steps (in exact arithmetic)
	\item good solutions for $j\ll n$ 
	\item convergence
	$ \gamma_j \approx \left| {\frac {\sqrt{\kappa}-1}{\sqrt{\kappa}+1}} \right|^j $
\end{itemize}


\end{frame}

\begin{frame}[fragile]
	\frametitle{CGLS: Conjugate Gradient Least-Squares}
\begin{verbatim}
     function x = cgls(A,c,k)
     n = size(A,2);
     x = zeros(n,1);
     d = A'*c;   r = c;
     normr2 = d'*d;
     for j=1:k
         Ad = A*d; alpha = normr2/(Ad'*Ad);
         x  = x + alpha*d;
         r  = r - alpha*Ad;
         d  = A'*r;
         normr2New = d'*d;
         beta = normr2New/normr2;
         normr2 = normr2New;
         d = d + beta*d;
     end
\end{verbatim}



\end{frame}

\begin{frame}
	\frametitle{Conjugate Gradient Least-Squares}
\begin{columns}
	\column{.7\textwidth}
	\begin{itemize}
	\item
	Uses the structure of the problem to obtain stable implementation
	\item
	Typically converges much faster than SD
	\item 
	Accelerate using post conditioning
	$$ \min_\bfx \hf \|\bfA \bfS \bfS^{-1} \bfx - \bfc\|^2$$
	\item 
	Faster convergence when eigenvalues of $\bfS^{\top} \bfA^{\top} \bfA \bfS$ are clustered.
	\end{itemize}
	
	\column{.3 \textwidth}
		
	\includegraphics[width=38mm]{exConjugateGradient2D}
\end{columns}
\end{frame}

\begin{frame}
	\frametitle{Iterative Regularization}

Consider
$$
	\min_\bfx \| \bfA \bfx - \bfb \|^2
$$
\begin{itemize}
\item
Assume that $\bfA$ has non-trivial null space
\item The matrix $\bfA^{\top}\bfA$ is not invertible
\item Can we still use iterative methods (CG, CGLS, \ldots)?
\end{itemize}

\bigskip

What are the properties of the iterates? 

\bigskip

Excellent introduction to computational inverse problems~\cite{Hansen1998,Vogel2002,Hansen2010}

\end{frame}




\begin{frame}
	\frametitle{Iterative Regularization: L-Curve}


The CGLS algorithm has the following properties
\begin{itemize}
\item For each iteration $\| \bfA \bfx_k -\bfc \|^2 \le \| \bfA \bfx_{k-1} -\bfc \|^2$
\item If starting from  $\bfx=0$ then $\|\bfx_k  \|^2 \ge \|  \bfx_{k-1}  \|^2$

\item $\bfx_1,\bfx_2,\ldots$ converges to the minimum norm solution of the problem
\item Plotting $\|\bfx_k\|^2$ vs $\| \bfA\bfx_k -\bfc \|^2$ typically has the shape of an L-curve
\end{itemize}



\begin{center}
\includegraphics[width=6cm]{lcurve.jpg}
\end{center}
\end{frame}


\begin{frame}
	\frametitle{Cross Validation - 1}

Finding good least-squares solution requires good parameter selection.
\begin{itemize}
\item $\lambda$ when using Tikhonov regularization (weight decay)
\item number of iteration (for SD and CGLS)
\end{itemize}

\bigskip

Suppose that we have two different ``solutions''

$$\bfx_1\quad  \rightarrow \quad  \|\bfx_1\|^2 = \eta_1 \quad \|\bfA \bfx_1 - \bfc\|^2 = \rho_1. $$
$$\bfx_2\quad  \rightarrow \quad  \|\bfx_2\|^2 = \eta_2 \quad \|\bfA \bfx_2 - \bfc\|^2 = \rho_2. $$

How to decide which one is better?

\end{frame}

\begin{frame}
	\frametitle{Cross Validation - 2}

Goal: Gauge how well the model can predict new examples.

\bigskip

Let $\{ \bfA_{\rm CV}, \bfc_{\rm CV} \}$ be data that is {\bf not used} for the training 

\bigskip

Idea: If $\|\bfA_{\rm CV} \bfx_1 - \bfc_{\rm CV}\|^2 \le 
\|\bfA_{\rm CV} \bfx_2 - \bfc_{\rm CV}\|^2$,
then $\bfx_1$ is a better solution that $\bfx_2$.

\bigskip

\pause

When the solution depends on some hyper-parameter(s) $\lambda$, we can phrase this as bi-level optimization problem
\begin{equation*}
 \lambda^* = \argmin_{\lambda} \|\bfA_{\rm CV} \bfx(\lambda) - \bfc_{\rm CV}\|^2,
\end{equation*}
where  $\bfx(\lambda) = \argmin_\bfx \|\bfA \bfx - \bfx \|^2 + \lambda \|\bfx\|^2$.

\end{frame}

\begin{frame}
	\frametitle{Cross Validation - 3}

To assess the final quality of the solution cross validation is not sufficient (why?).

\bigskip

Need a final testing set.

\bigskip

Procedure
\begin{itemize}
\item Divide the data into 3 groups $\{ \bfA_{\rm train}, \bfA_{\rm CV}, \bfA_{\rm test} \}$.
\item Use $\bfA_{\rm train}$ to estimate $\bfx(\lambda)$
\item Use $\bfA_{\rm CV}$ to estimate $\lambda$
\item Use $\bfA_{\rm test}$ to assess the quality of the solution
\end{itemize}

\pause

{\bf Important} -  we are not allowed to use $\bfA_{\rm test}$ to tune parameters!

\end{frame}




\begin{frame}[fragile]
	\frametitle{Coding: Iterative Methods for Regression}
	
	Outline:
	\begin{itemize}
		\item Dataset: MNIST / CIFAR 10
		\item write a steepest descent for least-squares problems
		\begin{verbatim}
			function x = sdLeastSquares(A,c,x0,maxIter)
		\end{verbatim}
		\item write a conjugate gradient code
		\begin{verbatim}
			function x = cgLeastSquares(A,c,maxIter)
		\end{verbatim}
	\end{itemize}
\end{frame}



\begin{frame}[allowframebreaks]
	\frametitle{References}
\bibliographystyle{abbrv}
\bibliography{NumDNN}

\end{frame}
\end{document}
