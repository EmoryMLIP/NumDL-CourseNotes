\documentclass[12pt,fleqn,handout]{beamer}

\input{beamerStyle.tex}
\input{abbrv.tex}


\title{ One-Layer Neural Networks}
\subtitle{Numerical Methods for Deep Learning}
\date{
}
\begin{document}

\makebeamertitle

\input{slides/introNonlinearModels.tex}

\begin{frame}[fragile]\frametitle{Learning the Weights}

Assume that the number of examples, $n$, is very large.

Using random weights, $\bfK$ might need to be very large to fit training data.

Solution may not generalize well to test data.

\bigskip
\pause

Idea: Learn $\bfK$ and $b$  from the data (in addition to $\bfW$)

$$ \min_{\bfK,\bfW,b} E(\bfW\sigma(\bfK \bfY + b), \bfC^{\rm obs}) + \lambda R(\bfW,\bfK,\bfb)$$

About this optimization problem:
\begin{itemize}
	\item more unknowns $\bfK \in \R^{m \times n_f}$, $\bfW \in \R^{n_c \times m}$, $b \in \R$
	\item  non-convex problem $\leadsto$ local minima, careful initialization
	\item need to compute derivatives w.r.t. $\bfK, b$
\end{itemize}


\end{frame}

\begin{frame}
	\frametitle{Non-Convexity}
	The optimization problem is non-convex. Simple illustration of cross-entropy along two random directions $d\bfK$ and $d\bfW$

	\begin{center}
		\includegraphics[width=.6\textwidth]{images/nonConvexitySingleLayer}
		
		(see \texttt{E08SingleLayerNN.m})
	\bigskip
	
	Expect worse when number of layers grows!
	\end{center}

\end{frame}
\begin{frame}[fragile]\frametitle{Training the Neural Network}

\begin{itemize}
\item If non-convexity is not ``too bad'' can use standard gradient based methods
\item If non-convexity is ``ugly'' need to modify standard methods (stochastic kick)
\item If non-convexity is ``bad'' need global optimization techniques
\end{itemize}

	\begin{center}
	\begin{tabular}{ccc}
		\includegraphics[width=.3\textwidth]{images/goodConv} &
		\includegraphics[width=.3\textwidth]{images/badConv} &
		\includegraphics[width=.3\textwidth]{images/uglyConv} \\
		good & bad & ugly \\
		\end{tabular}
		\end{center}



\end{frame}
\begin{frame}[fragile]\frametitle{Recap: Differentiating Linear Algebra Expressions}

Easy ones:
\begin{align*}
 F_1(\bfx,\bfy) &= \bfx^{\top} \bfy  & \bfJ_{\bfx}F_1(\bfx,\bfy) = \bfy^\top\\
F_2(\bfA,\bfx)  &= \bfA \bfx         & \bfJ_{\bfx}F_2(\bfx,\bfy) = \bfA 
\end{align*}
\pause
How about
$$ F_3(\bfA,\bfX) =   \bfA  \bfX \quad \quad \bfJ_{{\rm vec}(\bfX)}F_3 = ??? $$
\pause
Recall that
$${\rm vec}(\bfA \bfX) = {\rm vec}(\bfA  \bfX \bfI) = (\bfI \otimes \bfA) {\rm vec} (\bfX) $$
Therefore:
$$ \bfJ_{{\rm vec}(\bfX)}F_3(\bfA,\bfX) = \bfI \otimes \bfA $$
\pause
\textcolor{red}{
Efficient mat-vec: } $\bfJ_{{\rm vec}(\bfX)}F \bfv = {\rm vec} ( { \bfA\ \rm mat}(\bfv) )$
	
\end{frame}


\begin{frame}[fragile]\frametitle{Training Single Layer Neural Network}

Assume no regularization (easy to add) and re-write optimization problem as 
\begin{eqnarray*}
 \min_{\bfW,\bfK,b}  E(\bfC^{\rm obs}, \bfZ,\bfW ) \quad \text{ with} \quad \bfZ = \sigma(\bfK\bfY + b)
\end{eqnarray*}

\bigskip
\pause

Agenda:
\begin{enumerate}
	\item compute derivative of ${\rm vec}(\bfZ)$ w.r.t. ${\rm vec}(\bfK), b$ 
	\item use chain rule to get
	\begin{align*}
		\bfJ_{{\rm vec}(\bfK)} E  & = \bfJ_{{\rm vec}(\bfZ)} E(\bfC^{\rm obs},\bfZ,\bfW) \ \bfJ_{{\rm vec}(\bfK)} \bfZ\\
		\bfJ_{b} E  & = \bfJ_{{\rm vec}(\bfZ)} E(\bfC^{\rm obs},\bfZ,\bfW) \ \bfJ_{b} \bfZ
	\end{align*}
	\item efficient code for mat-vecs with $\bfJ$ and $\bfJ^\top$
\end{enumerate} 
\end{frame}

\begin{frame}[fragile]\frametitle{Computing Jacobians}
$$
\bfZ = \sigma(\bfK\bfY + b)
$$
Recall that $\sigma$ is applied element-wise.
$$
	\bfJ_{{\rm vec}(\bfK)}\bfZ = {\rm diag}(\sigma'(\bfK \bfY + b)) (\bfY^\top \otimes \bfI)
$$
\pause
Efficient way to get matrix vector products
\begin{eqnarray*}
\bfJ_{{\rm vec}(\bfK)}\bfZ \bfv &=& {\rm diag}(\sigma'(\bfK \bfY + b)) (\bfY^\top \otimes \bfI)\bfv\\
                            &=& {\rm vec} \left(\sigma'(\bfK \bfY + b) \odot ( {\rm mat}(\bfv) \bfY )\right)
\end{eqnarray*}
And for transpose get
\begin{eqnarray*}
(\bfJ_{{\rm vec}(\bfK)}\bfZ)^\top \bfu &=& (\bfY \otimes \bfI) {\rm diag}(\sigma'(\bfK \bfY + b)) \bfu\\
                            &=& {\rm vec} \left(\sigma'(\bfK \bfY + b) \odot  {\rm mat}(\bfu)\bfY^\top\right) 
\end{eqnarray*}

\end{frame}




\begin{frame}[fragile]\frametitle{Class Problems: Derivatives of Single Layer}

\textbf{Derivations:}
\begin{enumerate}
	
	\item Compute $\bfJ_{b}\bfZ v$ and $(\bfJ_{b}\bfZ)^\top \bfu$

	\item Compute $\bfJ_{{\rm vec}(\bfY)}\bfZ  \bfv$ and $(\bfJ_{{\rm vec}(\bfY)}\bfZ)^\top  \bfu$
\end{enumerate}

\textbf{Coding:}
\begin{verbatim}
function[Z,JKt,Jbt,JYt,JK,Jb,JY] = singleLayer(K,b,Y)
% Returns Z = sigma(K*Y+b) and 
%                         functions for J'*U and J*V
\end{verbatim}

\textbf{Testing:}
\begin{enumerate}
	\item Derivative check for Jacobian mat-vec
	\item Adjoint tests for transpose, let $\bfv,\bfu$ be arbitray vectors
	$$
		\bfu^\top \bfJ \bfv \approx \bfv^\top \bfJ^\top \bfu
	$$
\end{enumerate}

\end{frame}


\begin{frame}[fragile]
	\frametitle{Putting Things Together}
	
	Implement loss function of single-layer NN
	$$
		E(\bfK,b,\bfW) \stackrel{def}{=} E(\bfC,\bfZ,\bfW), \quad \bfZ = \sigma(\bfK \bfY+b)
	$$
	
	
	\begin{verbatim}
		function [Ec,dE] = singleLayerNNObjFun(x,Y,C,m)
		% where x = [K(:); b; W(:)]
		% evaluates single layer and computes cross entropy 
		%        and gradient (extend for approx. Hessian)
	\end{verbatim}

	\bigskip
	
    Use
	\begin{enumerate}
		\item $\nabla_{\bfZ} E =  \bfW^\top \nabla_{\bfS} \ E(\bfS), \quad \bfS = \bfW\bfZ$
		\item $\nabla_{\bfK} E =  \bfJ_{\bfK}^\top \nabla_{\bfZ} E$
		\item $\nabla_{\bfb} E =  \bfJ_{\bfb}^\top \nabla_{\bfZ} E$
		\item $\nabla_{\bfW} E =  \nabla_{\bfS} \ E(\bfS) \bfY $
	\end{enumerate}
	
	
\end{frame}

\begin{frame}[fragile]
	\frametitle{Test Problem}
	
	Before going to real data, let us try the \emph{inverse crime}. Generate data
	\begin{verbatim}
		n  = 500; nf = 50; nc = 10; m  = 40;
		Wtrue = randn(nc,m);
		Ktrue = randn(m,nf);
		btrue = .1;

		Y     = randn(nf,n);
		Cobs  = exp(Wtrue*singleLayer(Ktrue,btrue,Y));
		Cobs  = Cobs./sum(Cobs,1);		
	\end{verbatim}
	
	\begin{center}
	Goal: Reconstruct \texttt{Wtrue, Ktrue, btrue}!  
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{Gauss-Newton Method}
	
	\textbf{Goal:} Accelerate convergence by using curvature information.
	$$
	\nabla_{\bfK} E(\bfK,b,\bfW) =  (\bfJ_\bfK \bfZ)^\top \nabla_\bfZ E(\bfW\sigma(\bfK \bfY+b),\bfC),
	$$
	Denoting $\bfJ_\bfK \bfZ = \nabla_{\bfK} \sigma(\bfK \bfY+b)^\top$ this means that Hessian is
	\begin{equation*}
		\begin{split}
	\nabla_{\bfK}^2 E(\bfK) &  = (\bfJ_{\bfK}\bfZ)^\top  \nabla_\bfZ^2 E(\bfC,\bfZ,\bfW) \bfJ_{\bfK}\bfZ\\
	 & + \sum_{i=1}^{n}\sum_{j=1}^{m} \nabla_{\bfK}^2 \sigma(\bfK \bfY+b)_{ij} \nabla_\bfZ E(\bfC,\bfZ,\bfW)_{ij}
		\end{split}
	\end{equation*}
	
	First term is spsd and we can compute it.
	
	We neglect second term since
	\begin{itemize}
		\item can be indefinite and difficult to compute
		\item small if transformation is roughly linear or close to solution (easy to see for least-squares)
	\end{itemize}
	\begin{center}
		\textcolor{red}{inexact Hessian $+$ inexact solve: add line search!}
	\end{center}	
\end{frame}

\begin{frame}[fragile]
	\frametitle{Experiment: Adversarial Example}
	
	Suppose you have trained your network $\leadsto \bfK, b, \bfW$ so that validation loss is low. This means that for most examples $\bfy$, 
	$$
		\bfW \sigma(\bfK \bfy + b) \approx \bfc.
	$$
	
	An adversary might try to fool this classifier by adding a small perturbation $\bfd$ to the example to achieve a desired label $\hat{\bfc}$. 
	
	\bigskip
	
	Formulate as optimization problem
	$$
		\min_{\bfd} E(\bfW \sigma(\bfK (\bfy + \bfd) + b), \hat{\bfc})
	$$
	\begin{itemize}
		\item setup objective function
		\item think about constraints, regularization
	\end{itemize}
\end{frame}


\end{document}
\begin{frame}[fragile]\frametitle{Learning the weights}

{\bf Class problem}

\bigskip


Code a function
\small
\end{frame}


\begin{frame}[allowframebreaks]
	\frametitle{References}
\bibliographystyle{abbrv}
\bibliography{NumDNN}

\end{frame}



\end{document}



































\begin{frame}[fragile]\frametitle{Why deep models}

\begin{itemize}
\item
Fundamental theorem of NN suggests that we can fit {\bf any} data by
two layers.
\item
But - The width of the layer can be very large ${\cal O}(n_d)$ where
$n_d$ is the dimension of the data.
\item
Deeper architectures can lead to more efficient descriptions of the problem.
\item
No real proof but lots of practical experience.
\end{itemize}


\end{frame}

\begin{frame}[fragile]\frametitle{Deep models}

How deep is deep?

We will answer this question later ...

\bigskip

Until recently, the architecture was
\begin{eqnarray*}
\bfY_2 &=& \sigma(Y_1\bfK_1 + b_1) \\
 &&  ... \\
 \bfY_N &=& \sigma( \bfY_{N-1}\bfK_{N-1} + b_{N-1})
 \end{eqnarray*}

And use $\bfY_N$ to classify
$$ \min \ \ E\left(\bfY_N(\bfK_1,\ldots,\bfK_{N-1},b_1,\ldots, b_{N-1})\bfW , \bfC^{\rm obs} \right) $$

\end{frame}


\begin{frame}[fragile]\frametitle{Deep models}


\begin{itemize}
\item
Architecture has many (billions) of parameters.
\item
Very difficult to design
\item
Strange computational behavior
\item
Very unpredictable
\end{itemize}

\bigskip

In 2015 He et-al came with a new architecture that solves many of the problems


\end{frame}

\begin{frame}[fragile]\frametitle{Deep models}

Residual Network

\bigskip

\begin{eqnarray*}
\bfY_2 &=& \bfY_1 + \sigma(Y_1\bfK_1 + b_1) \\
 &&  ... \\
 \bfY_N &=& \bfY_{N-1} + \sigma( \bfY_{N-1}\bfK_{N-1} + b_{N-1})
 \end{eqnarray*}

And use $\bfY_N$ to classify
$$ \min \ \ E\left(\bfY_N(\bfK_1,\ldots,\bfK_{N-1},b_1,\ldots, b_{N-1})\bfW , \bfC^{\rm obs} \right) $$

\bigskip

Much more stable!

\end{frame}

\begin{frame}[fragile]\frametitle{Deep models - Residual Network}

Why ResNets can be stable?

A small change
\begin{eqnarray*}
\bfY_2 &=& \bfY_1 + h\sigma(Y_1\bfK_1 + b_1) \\
 &&  ... \\
 \bfY_N &=& \bfY_{N-1} + hf( \bfY_{N-1}\bfK_{N-1} + b_{N-1})
 \end{eqnarray*}

This is nothing but a forward Euler discretization
of the  ODE of the form
$$ \dot{\bfY} = f(\bfY \bfK(t) + b(t)) $$

We can understand the behavior by learning the dynamics of nonlinear evolution equation.

\end{frame}

\begin{frame}[fragile]\frametitle{A crash review on ODE's}

Given the ODE
$$ \dot{\bfy} = f(t,\bfy) $$
Assume $f$ differentiable with
$$ \bfJ(t,\bfy) = {\frac {\partial f}{\partial \bfy}} $$

Then
\begin{itemize}
\item If $Re({\rm eig}(\bfJ)) > 0\quad \quad \rightarrow$\ Unstable
\item If $Re({\rm eig}(\bfJ)) < 0\quad \quad \rightarrow$\ Stable (converge to a stationary point)
\item If $Re({\rm eig}(\bfJ)) = 0\quad \quad \rightarrow$\ Stable, energy bounded
\end{itemize}


\end{frame}


\begin{frame}[fragile]\frametitle{Residual Network as a path planning problem}


$$ \dot{\bfY} = f(\bfY \bfK + b) \quad \bfY(0) = \bfY_0 $$


\movie[autostart,repeat,height=4.0cm,poster,showcontrols=true,width=9cm]{}{dynamics.avi}



The goal is to plan a path such that the initial data can be linearly separated


\end{frame}



\begin{frame}[fragile]\frametitle{Residual Network as a path planning problem}


$$ \dot{\bfY} = f(\bfY \bfK + b) \quad \bfY(0) = \bfY_0 $$

The goal is to plan a path such that the initial data can be linearly separated


\movie[autostart,repeat,height=4.0cm,poster,showcontrols=true,width=9cm]{}{dynamicsClassify.avi}




\end{frame}



\begin{frame}[fragile]\frametitle{Residual Network as a path planning problem}


$$ \dot{\bfY} = f(\bfY \bfK + b) \quad \bfY(0) = \bfY_0 $$

The goal is to plan a path such that the initial data can be linearly separated


\begin{center}
\includegraphics[width=7cm]{circleBeforeAfter.jpg}
\end{center}


\end{frame}

\begin{frame}[fragile]\frametitle{Residual Network - solving the problem}


$$ \dot{\bfY} = f(\bfY \bfK + b) \quad \bfY(0) = \bfY_0 $$

Propagation = Forward Euler method
\begin{eqnarray*}
\bfY_{j+1} = \bfY_j + hf(\bfY_j \bfK_j + b_j)
\end{eqnarray*}
$\bfY_j$ - state

\bigskip

Classification with last state by minimizing
$$ \min_{\bfW, \bfK_{1,\ldots,N},b_{1,\ldots,N}}{\cal S}\left(
\bfY_N(\bfK_{1,\ldots,N},b_{1,\ldots,N})\bfW,\bfC^{\rm obs}\right) $$


\end{frame}

\begin{frame}[fragile]\frametitle{Computing derivatives - the adjoint method}

Differentiating $\grad_{\bfW} {\cal S}$ as done before.

\bigskip

Differentiating $\grad_{\bfK_j} {\cal S}$ using the chain rule
and the adjoint method.

\bigskip

Fist, note that

$$ \grad_{\bfK_j} {\cal S} =  \left(\grad_{\bfK_j} {\bfY_N}\right)^{\top} \grad_{\bfY_N} {\cal S}
$$

Need to compute
$\left(\grad_{\bfK_j} {\bfY_N}\right)^{\top}$
\end{frame}

\begin{frame}[fragile]\frametitle{Computing derivatives - the sensitivity equation}

We differentiate the stepping directly to obtain
{\small
\begin{eqnarray*}
\grad_{\bfK_j}\bfY_{n+1} = \grad_{\bfK_j}\bfY_n + h {\rm diag}(f(\bfY_n \bfK_n + b_n))
(\bfK_n^{\top} \otimes \bfI) \grad_{\bfK_j}\bfY_n
\end{eqnarray*}}

\bigskip

This is a block triangular {\bf linear} system for the gradients

\end{frame}


\begin{frame}[fragile]\frametitle{Computing derivatives - the sensitivity equation}

Block triangular {\bf linear} system for the gradients
{\small
\begin{eqnarray*}
\begin{pmatrix}
\bfI              &                &                &          &       \\
\bfT_{j+1}    &   \bfI       &                &          &       \\
                    & \ddots    &  \ddots    &          &      \\
                    &     &      &          &      \\
                    &     &        &   \bfT_{N-1}       & \bfI
                    \end{pmatrix}
                    \begin{pmatrix}
                    \grad_{\bfK_j}\bfY_{j+1} \\    \\   \\ \\   \grad_{\bfK_j}\bfY_{N}
                    \end{pmatrix} =
                    \begin{pmatrix}
                    \grad_{\bfK_j}f_j \\    \\   \\ \\   \\
                    \end{pmatrix}
\end{eqnarray*}}


$$\bfT_n = \bfI + h {\rm diag}(f(\bfY_n \bfK_n + b_n))
(\bfK_n^{\top} \otimes \bfI) $$

%To multiply $ \grad_{\bfK_j}\bfY_{N}\bfv$

\end{frame}



\begin{frame}[fragile]\frametitle{Computing derivatives - the sensitivity equation}

Block triangular {\bf linear} system for the gradients
{\small
\begin{eqnarray*}
&&
\begin{pmatrix}
\bfI              &                &                &          &       \\
\bfT_{j+1}    &   \bfI       &                &          &       \\
                    & \ddots    &  \ddots    &          &      \\
                    &     &      &          &      \\
                    &     &        &   \bfT_{N-1}       & \bfI
                    \end{pmatrix}
                    \begin{pmatrix}
                    \grad_{\bfK_j}\bfY_{j+1} \\    \\   \\ \\   \grad_{\bfK_j}\bfY_{N}
                    \end{pmatrix} =
                    \begin{pmatrix}
                    \grad_{\bfK_j}f_j \\    \\   \\ \\   \\
                    \end{pmatrix}   \\
 &&        \quad \quad   \quad \quad   \quad \quad        \bfT \quad\quad \quad\quad\quad \quad \quad \grad_{\bfK_j}\bfY \quad = \quad \quad \grad_{\bfK_j}f
\end{eqnarray*}}

To compute matrix-vector $\grad_{\bfK_j}\bfY_{N} \bfv$
\begin{itemize}
\item Multiply $\grad_{\bfK_j}f \bfv$
\item Solve (forward propagate) $\bfT \grad_{\bfK_j}\bfY_N = \grad_{\bfK_j}f \bfv$
\item Extract the last time step
\end{itemize}

\end{frame}

\begin{frame}[fragile]\frametitle{The sensitivity equation}


Symbolically

$$ \grad_{\bfK_j}\bfY_{N}  = \bfP_N \bfT^{-1}  \grad_{\bfK_j}f  $$
where
$$\bfP = [0,\ldots,\bfI] $$.


\bigskip

The transpose

$$ \left(\grad_{\bfK_j}\bfY_{N}\right)^{\top} =  (\grad_{\bfK_j}f)^{\top} \bfT^{-T} \bfP^{\top} $$


\end{frame}

\begin{frame}[fragile]\frametitle{The sensitivity equation}

$$ \left(\grad_{\bfK_j}\bfY_{N}\right)^{\top} =  (\grad_{\bfK_j}f)^{\top} \bfT^{-T} \bfP^{\top} $$


{\small
\begin{eqnarray*}
\begin{pmatrix} (\grad_{\bfK_j}f_j)^{\top}  &   0   &  \hdots     &   & 0 \end{pmatrix}
\begin{pmatrix}
\bfI              &  \bfT_{j+1}^{\top}  &                &          &       \\
                    &   \bfI       &     \bfT_{j+2}^{\top}           &          &       \\
                    &     &  \ddots    &    \ddots      &      \\
                    &     &      &    \bfI      &  \bfT_{N-1}    \\
                    &     &        &          & \bfI
                    \end{pmatrix}
  \begin{pmatrix} 0 \\    \\   \vdots  \\    \\  \bfI \end{pmatrix}
\end{eqnarray*}
}


To multiply by the transpose
\begin{itemize}
\item Initialize with last step
\item {\bf solve backward} in time
\item Extract the first step and multiply by $(\grad_{\bfK_j}f_j)^{\top} $
\end{itemize}

\end{frame}

\begin{frame}[fragile]\frametitle{More about the sensitivity equation}

To compute $(\grad_{\bfK_j}f_j)^{\top} $ for all $j$'s note that the same quantities
are recomputed. Can be evaluated in ${\cal O}(n_t)$

\bigskip

For gradient based method the transpose is sufficient

\bigskip

Newton based methods require both forward sensitivities and adjoint.


\end{frame}

\begin{frame}[fragile]\frametitle{More about the sensitivity equation}

{\bf Programming the adjoint - the adjoint test}

Code a - Computes $(\grad_{\bfK_j}\bfY_N)\bfv$

Code b - Computes $(\grad_{\bfK_j}\bfY_N)^{\top}\bfu$

\bigskip

Testing - for random $\bfu,\bfv$
$$ \bfu^{\top} \left((\grad_{\bfK_j}\bfY_N)\bfv \right) = \bfv^{\top} \left((\grad_{\bfK_j}\bfY_N)^{\top}\bfu \right)
$$

\end{frame}

\begin{frame}[fragile]\frametitle{Back to the optimization problem}

$$ \min \ \ E\left(\bfY_N(\bfK_1,\ldots,\bfK_{N-1},b_1,\ldots, b_{N-1})\bfW , \bfC^{\rm obs} \right) $$

\bigskip

{\bf gradient}

$$ \grad E = \begin{pmatrix} \bfY_N^{\top}\grad_{\bfW} E \\
 (\grad_{\bfK_1} \bfY_N)^{\top} \grad_{\bfY_N} E \\
\vdots \\
(\grad_{\bfK_{N-1}} \bfY_N)^{\top} \grad_{\bfY_N} E \\
\end{pmatrix} $$


Use gradient based techniques for the optimization

\end{frame}

\begin{frame}[fragile]\frametitle{Problem setup}

For most problems we use both width and depth.

\bigskip

{\bf Input} \quad \quad \quad \quad {\bf hidden} \quad \quad \quad \quad \quad {\bf output}

\bigskip

\begin{tikzpicture}
\filldraw [blue] (0,1) circle (10pt);
\filldraw [blue] (0,-1) circle (10pt);

\filldraw [red] (2,2) circle (10pt);
\filldraw [red] (2,1) circle (10pt);
\filldraw [red] (2,0) circle (10pt);
\filldraw [red] (2,-1) circle (10pt);
\filldraw [red] (2,-2) circle (10pt);

\filldraw [red] (4,2) circle (10pt);
\filldraw [red] (4,1) circle (10pt);
\filldraw [red] (4,0) circle (10pt);
\filldraw [red] (4,-1) circle (10pt);
\filldraw [red] (4,-2) circle (10pt);

\filldraw [red] (6,2) circle (10pt);
\filldraw [red] (6,1) circle (10pt);
\filldraw [red] (6,0) circle (10pt);
\filldraw [red] (6,-1) circle (10pt);
\filldraw [red] (6,-2) circle (10pt);

\filldraw [green] (8,1) circle (10pt);
\filldraw [green] (8,-1) circle (10pt);
\filldraw [green] (8,0) circle (10pt);


\end{tikzpicture}

\end{frame}

\begin{frame}[fragile]\frametitle{Problem setup}

For most problems we use both width and depth.

\bigskip

How wide should we go?


\bigskip

How deep should we go?

Better understanding of depth vs width.

\end{frame}

\begin{frame}[fragile]\frametitle{Problem setup}

More about width
\begin{itemize}
\item Use (initialize) by filters
\item Domain specific knowledge  (e.g. optical flow)
\item In general open
\end{itemize}


\end{frame}


\begin{frame}[fragile]\frametitle{Class problem - DNN}

\begin{itemize}
\item ResNN.m  -  propagate the network forward
\item dResNNTmatVec.m  - derivative of the network with respect to parameters
\item softMax.m -
\item newton.m - Solve the classification problem
\end{itemize}

Put it together to solve 2D classification problems circle, peaks and spiral

Plot a movie - the points in time


\end{frame}


\begin{frame}[fragile]\frametitle{}

{\bf {\Huge Part IV \\  parametric  models}}

\end{frame}

\begin{frame}[fragile]\frametitle{parametric models}

Number of parameters - for a resNet with $n$ parameters

$$ n^2 \times n_t \quad + \quad n_t \quad + \quad n \times n_f$$
\begin{itemize}
\item $n$ - size of the matrix
\item $n_t$ - number of times
\item $n_f$ - number of features
\end{itemize}

Simple example - MNIST $n = 28^2$ pixels

with 10 layers we have millions of parameters

\end{frame}

\begin{frame}[fragile]\frametitle{parametric models}

To reduce the effective degrees of freedom
\begin{itemize}
\item Regularization - penalize some function of the model

\item Parametrization - $\bfK = \bfK(\bftheta)$ where
${\rm dim}(\bftheta) \ll {\rm dim}(\bfK)$
\end{itemize}




\end{frame}

\begin{frame}[fragile]\frametitle{parametric models convolution}

\begin{itemize}
\item
Assume that $\bfY_0$ are images/videos of size
$n_x \times n_y \times n_z \times n_c$.
\item 
For images $n_x$ and $n_y$ are the number of pixels in $x$ and $y$ direction
\item 
For videos $n_x$ and $n_y$ are the number of pixels in $x$ and $y$ direction
and $n_z$ are the number of frames
\item $n_c$ is the color dimension
\end{itemize}


Size of every image/video can be very large

\end{frame}

% \begin{frame}[allowframebreaks]
% 	\frametitle{References}
% \bibliographystyle{abbrv}
% \bibliography{NumDNN}
%
% \end{frame}

\end{document}
















